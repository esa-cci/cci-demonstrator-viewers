{"version":"1","records":[{"hierarchy":{"lvl1":"cci-demonstrator-viewers"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"cci-demonstrator-viewers"},"content":"https://​esa​-cci​.github​.io​/cci​-demonstrator​-viewers/","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"ect-demonstrators"},"type":"lvl1","url":"/external-notebooks/esa-cci/ect-demonstrators/readme","position":0},{"hierarchy":{"lvl1":"ect-demonstrators"},"content":"A repository containing demonstrators, showing how to run the Toolbox with different types of technologies","type":"content","url":"/external-notebooks/esa-cci/ect-demonstrators/readme","position":1},{"hierarchy":{"lvl1":"Airflow Demonstrator"},"type":"lvl1","url":"/external-notebooks/esa-cci/ect-demonstrators/demonstrators/airflow/readme","position":0},{"hierarchy":{"lvl1":"Airflow Demonstrator"},"content":"This demonstrator has been tested on a Linux environment.\n\nIt shows how Apache Airflow can be used to run a workflow including functionality from the ESA Climate Toolbox.\nAs stated on their \n\nwebsite, Apache Airflow is a platform created by the community to programmatically author, schedule and monitor workflows.\nWorkflows are defined once and may be rerun multiple times.\nThey come in the form of Directed Acyclic Graphs (DAGs).\nFor this demonstrator, we have set up such a dag, which can be seen in the file dag_config.yaml in the dags folder.\nYou can read more about specifics of how a configuration file can be set up \n\nhere.\nWe explain the particularities of the DAG in the following section, if you want to run the demonstrator immediately, go to \n\nPrerequisites.","type":"content","url":"/external-notebooks/esa-cci/ect-demonstrators/demonstrators/airflow/readme","position":1},{"hierarchy":{"lvl1":"Airflow Demonstrator","lvl2":"The Directed Acyclic Graph (DAG)"},"type":"lvl2","url":"/external-notebooks/esa-cci/ect-demonstrators/demonstrators/airflow/readme#the-directed-acyclic-graph-dag","position":2},{"hierarchy":{"lvl1":"Airflow Demonstrator","lvl2":"The Directed Acyclic Graph (DAG)"},"content":"The dag is specified in the section esa_climate_toolbox_ops_dag. This is the name under which the dag will later be found in the UI.\n\nIn this section, there are sub-sections task_groups and tasks.\nTask Groups are helpful to group tasks, which can come in handy for large processes.\nHere, we only specify one group, ect_tasks.\nIn tasks, we dfine the actual sub processes.\nThere are three of them:\n\nchain runs a chain of ESA Climate Toolbox Operations on a dataset and writes the result to disk\n\nplot creates a plot from this result\n\ndelete_folder deletes the folder that included the data written by the chain task\n\nTasks chain and plot are Python tasks - and hence use a PythonOperator - whilst delete_folder merely executs a bash command, so it uses a BashOperator, as can be seen in the respective task’s entries for the field operator.\nBesides these, there is a large sum of operators, many of whom have been designed to cater to specific tasks (if you are interested, you can find a full list \n\nhere).\n\nPython operators must point to a specific Python function. This function can either be provided by a package (as is the case in the chain task, where we point to the execute_operations function from the ESA CCI Toolbox, which allows to run multiple operations consecutively (and which is documented \n\nhere)) or as a dedicated Python function, which we did for the plot task, where the function plot_chain_output is provided in the module plot_functions.py in the dags/functions folder.\nNote that it would have been possible to add the plotting operation to the chain, but for the purposes of this demonstrator, we wanted to show two different approaches.\nBoth Python operators receive their parameters from the op_kwargs sections in the tasks.\nFor the chain task, you see that we pick an aerosol dataset as input, create temporal, spatial, and thematic subsets, and ultimately write the result to an output_folder.\nThis provides a way to run ESA Climate Toolbox operations without interacting with Python directly.\nFor the plot task, we pick one of the variables of the dataset to plot and pick a name for the output file (for visualisation purposes, we here choose a region that is slightly larger than the subset region defined in chain).\nWhat’s further of importance to point out for the tasks is that it is here where we specify the pertinence of a task to a task group (in task_group_name) and define dependencies between tasks in dependencies.\nThe dependency [ chain ] in plot says that the task plot will not start before the task chain has finished.\nFinally, do_xcom_push is set to False, which means that no information is passed between tasks.\n\nThe last task, delete_folder, ultimately executes the bash command which deletes the folder with the output from chain. Remove this task if you want to keep the result.","type":"content","url":"/external-notebooks/esa-cci/ect-demonstrators/demonstrators/airflow/readme#the-directed-acyclic-graph-dag","position":3},{"hierarchy":{"lvl1":"Airflow Demonstrator","lvl2":"Prerequisites to run the Demonstrator"},"type":"lvl2","url":"/external-notebooks/esa-cci/ect-demonstrators/demonstrators/airflow/readme#prerequisites-to-run-the-demonstrator","position":4},{"hierarchy":{"lvl1":"Airflow Demonstrator","lvl2":"Prerequisites to run the Demonstrator"},"content":"To run this demonstrator, you need mamba (from \n\nhere), alternatively conda.\nWhen you have mamba configured, you can create a dedicated environment using the provided file environment.yml.\nTo do that, run    mamba env create -f environment.yml\n\nThis will install all the libraries required.\nNext, export the following environment variables    export AIRFLOW_HOME=$(pwd)\n    export AIRFLOW__CORE__LOAD_EXAMPLES=false\n\nThe first one will make the current folder the working directory, so that all outputs are written there.\nThe second one will prevent other dags from being loaded in, which makes it easier to keep an overview.\nNext, switch to the environment you had just created.    mamba activate airflow-ect\n\nYou can now run Airflow as a stand-alone server.    airflow standalone\n\nRunning this will create a long output. In this output, you will find a username (which is admin) and a password which changes on every restart.\nAlso, this will start an airflow server available at \n\nhttp://0.0.0.0:8080 .\nOpen this address in your browser, where you will be asked for the username and password.\nAfter providing the credentials, you will see the start screen, with the DAG esa_climate_toolbox_ops_dag loaded in.\nAt the far right, there is a green arrow which allows to trigger the DAG.\nIf you then click on the name of the DAG, you are brought to a page where you can see the state of processing.\nIn the graph view, you can see the individual tasks, grouped in the task group.\nIf you select one of the tasks and then Logs, you see information on the progress.\nIf all tasks have finished, in your working directory there will be a file airflow_plot.png showing aerosol optical thickness at 550 nm over Europe on May 20th, 2002.","type":"content","url":"/external-notebooks/esa-cci/ect-demonstrators/demonstrators/airflow/readme#prerequisites-to-run-the-demonstrator","position":5},{"hierarchy":{"lvl1":"Matching FIRE Data with Sentinel-2 Observations"},"type":"lvl1","url":"/external-notebooks/esa-cci/ect-demonstrators/demonstrators/cdse/matching-fire-data-with-sentinel-2-observations","position":0},{"hierarchy":{"lvl1":"Matching FIRE Data with Sentinel-2 Observations"},"content":"This notebook serves to demonstrate how data from the CCI FIRE ECV can be matched against single Sentinel-2 Observations provided by the Copernicus Data Space Ecosystem (CDSE). For this, we open the data sets separately and apply operations, some of whom provided by the ESA CCI Toolbox, to bring the data together.\n\nThis notebook runs in any Python environment which has the following packages installed:\n\nesa-climate-toolbox\n\nxcube-stac\n\njupyterlab\n\nlibgdal-netcdf\n\nFor convenience, there is an environment.yml in the folder next to this notebook you can use to create such an environment.\n\nWe start with necessary imports beforehand.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom xcube.core.gridmapping import GridMapping\nfrom xcube.core.resampling import resample_in_space\n\nfrom xcube.core.store import new_data_store\n\nfrom esa_climate_toolbox.core import get_op\nfrom esa_climate_toolbox.core import list_ecv_datasets\nfrom esa_climate_toolbox.core import open_data\n\n\n\n%matplotlib inline\n\n\n\nAs the CDSE provides data from the Copernicus Sentinel missions, it suggests itself to aim for combinations with data that is based on Sentinel missions. Using ‘list_ecv_datasets’, we may search for datasets for any ECV. We particularly want to look for FIRE data derived from Sentinel data. We may look for such datasets like this:\n\ndatasets = [(dataset, store) for dataset, store in list_ecv_datasets(\"FIRE\") if \"Sentinel\" in dataset]\ndatasets\n\n\n\nOf these, we pick the very first one, esacci.FIRE.mon.L4.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.grid, which is provided on a global grid.\n\nfire_ds, ds_name = open_data(\"esacci.FIRE.mon.L4.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.grid\")\nfire_ds\n\n\n\nWe now plot data from the very first timestep.\n\nsubset_time_index = get_op(\"subset_temporal_index\")\n\n\n\ntemp_sub_ds = subset_time_index(fire_ds, 0, 0)\ntemp_sub_ds.burned_area.plot()\n\n\n\nAs fire data only seems to be present in the center of the data, we select another subset, this time a spatial one.\n\nsubset_spatial = get_op(\"subset_spatial\")\n\n\n\nspat_temp_sub_ds = subset_spatial(temp_sub_ds, [28, 3, 35, 10])\nspat_temp_sub_ds.burned_area.plot()\n\n\n\nAs we now have a single image, we aim for matching it with an observation from the CDSE. For this, we may use the stac-cdse store, which has been installed from the xcube-stac plugin. The store requires credentials, please consider the readme for information on how to obtain them.\n\ncdse_credentials = dict(\n    key=<your_key>,\n    secret=<your_secret>\n)\n\n\n\nstac_cdse_store = new_data_store(\"stac-cdse\", **cdse_credentials)\n\n\n\nFor more information on how to use the xcube-stac store (and on how to use it also for non-CDSE data), see \n\nhttps://​github​.com​/xcube​-dev​/xcube​-stac​?tab​=​readme​-ov​-file​#xcube​-stac. For now, it should be enough to say that we can use the following code to search for observations from the same day that fall into the bounding box.\n\ndescriptors = list(\n    stac_cdse_store.search_data(\n        collections=[\"sentinel-2-l2a\"],\n        bbox=[28, 3, 35, 10],\n        time_range=[\"2019-01-16\", \"2019-01-16\"],\n    )\n)\n[d.to_dict() for d in descriptors]\n\n\n\nWe may now pick one of them, and we open bands B2, B3, and B4, which come in their 10m resolution.\n\nds = stac_cdse_store.open_data(\n    \"collections/sentinel-2-l2a/items/S2B_MSIL2A_20190116T082309_N0500_R121_T35NQF_20221202T191827\",\n    asset_names=[\"B04\", \"B03\", \"B02\"]\n)\nds\n\n\n\nAs this is a bit much for visualisation, we plot images where we only pick the tenth pixel from every row and column of the image.\n\nfig, ax = plt.subplots(1, 3, figsize=(20, 6))\nds.B02[::10, ::10].plot(ax=ax[0], vmin=0, vmax=0.2)\nds.B03[::10, ::10].plot(ax=ax[1], vmin=0, vmax=0.2)\nds.B04[::10, ::10].plot(ax=ax[2], vmin=0, vmax=0.2) \n\n\n\nNow we have the two datasets, but they are apparently provided with different spatial extents, even with different coordinate reference systems, which makes comparing the two hard. To easen this, we bring the datasets to the same grid. What we first need to do is to determine the gridmappings of the two datasets. Let’s assume here that we want to bring the data onto the ECV grid, so that grid is the target, whilst the S2 dataset provides the source.\n\nsource_gm = GridMapping.from_dataset(ds)\nsource_gm\n\n\n\ntarget_gm = GridMapping.from_dataset(spat_temp_sub_ds)\ntarget_gm\n\n\n\nNow this is defined, we can use xcube’s resample_in_space function (as introduced in the notebook showcasing how to reproject with the ESA CCI Toolbox) to bring the S2 Data to the other grid.\n\nresampled_ds = resample_in_space(\n    ds,\n    source_gm=source_gm,\n    target_gm=target_gm,\n)\nresampled_ds\n\n\n\nWe may now plot data from the resulting image.\n\nresampled_ds.B02.plot()\n\n\n\nLet’s try this the other way round: What would it look like if we’d map the fire data onto the S2 grid? To make this better to handle, we build a subset of the S2 grid, where we pick only every tenth pixel x- and y-wise.\n\ns2_ds = ds.isel({\"x\":slice(0, 10980, 10), \"y\":slice(0, 10980, 10)})\ns2_ds\n\n\n\nWe redefine the values for source_gm and target_gm ...\n\nsource_gm = target_gm\n\n\n\ntarget_gm = GridMapping.from_dataset(s2_ds)\ntarget_gm\n\n\n\nAnd now we may resample the s2 dataset to the fire dataset. (We squeeze it to get rid of the time dimension, which is unnecessary here, as the dataset only has one timestep).\n\nresampled_ds = resample_in_space(\n    spat_temp_sub_ds.squeeze(),\n    source_gm=source_gm,\n    target_gm=target_gm,\n)\nresampled_ds\n\n\n\n... and we may now reproject the data to see it is on the same grid as the S2 Data.\n\nresampled_ds.burned_area.plot()\n\n","type":"content","url":"/external-notebooks/esa-cci/ect-demonstrators/demonstrators/cdse/matching-fire-data-with-sentinel-2-observations","position":1},{"hierarchy":{"lvl1":"Matching LST Data with a Sentinel-3 Cube"},"type":"lvl1","url":"/external-notebooks/esa-cci/ect-demonstrators/demonstrators/cdse/matching-lst-data-with-a-sentinel-3-cube","position":0},{"hierarchy":{"lvl1":"Matching LST Data with a Sentinel-3 Cube"},"content":"This notebook serves to demonstrate how data from the CCI LST ECV can matched against an Analysis-Ready Data Cube created from Sentinel-3 Data and provided by the Copernicus Data Space Ecosystem (CDSE). For this, we open the data sets separately and apply operations, some of whom provided by the ESA CCI Toolbox, to bring the data together.\n\nThis notebook runs in any Python environment which has the following packages installed:\n\nesa-climate-toolbox\n\nxcube-stac\n\njupyterlab\n\nlibgdal-netcdf\n\nFor convenience, there is an environment.yml in the folder next to this notebook you can use to create such an environment.\n\nWe start with necessary imports beforehand.\n\nimport matplotlib.pyplot as plt\n\nfrom xcube.core.store import new_data_store\n\nfrom esa_climate_toolbox.core import get_op\nfrom esa_climate_toolbox.core import list_ecv_datasets\nfrom esa_climate_toolbox.core import open_data\n\n\n\n%matplotlib inline\n\n\n\nAs for the notebook focussed on finding matches with single observations, we are looking for data from the CDSE that is based on Sentinel missions. Using ‘list_ecv_datasets’, we may search for datasets for any ECV. We particularly want to look for LST data derived from Sentinel data. We may look for such datasets like this:\n\ndatasets = [(dataset, store) for dataset, store in list_ecv_datasets(\"LST\") if \"Sentinel\" in dataset]\ndatasets\n\n\n\nAs the Data cubes on the CDSE are provided in a daily resolution, we will pick one such dataset and open it. In particular, we want to examine extreme temperatures occurring in December 2019 in Southern Australia, so we set the region and time range accordingly when opening the dataset.\n\nsouthern_australia_event_region=[131.5, -32.5, 133.5, -30.5]\nsouthern_australia_event_time_range=[\"2019-12-15\", \"2019-12-20\"]\n\n\n\nlst_ds, ds_name = open_data(\n    \"esacci.LST.day.L3C.LST.SLSTR.Sentinel-3A.SLSTRA.3-00.DAY\",\n    region=southern_australia_event_region,\n    time_range=southern_australia_event_time_range,\n    var_names=\"lst\"\n)\nlst_ds\n\n\n\nWe continue with visualising all images at the same time.\n\ndef plot_six_images(ds, var_name):\n    fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(20, 6), sharex=True)\n    ax = ax.ravel()\n    for i in range(len(ds.time)):\n        ds[var_name].isel(time=i).plot(ax=ax[i])\n\n\n\nplot_six_images(lst_ds, \"lst\")\n\n\n\nWe have measurements for six conesecutive days, which is good. For purposes of conveying information, we transfer the data from Kelvin to Celsius using the ‘arithmetics’ operation.\n\narithmetics = get_op(\"arithmetics\")\n\n\n\nconverted_lst_ds = arithmetics(lst_ds, \"-272.15\")\nconverted_lst_ds\n\n\n\nplot_six_images(converted_lst_ds, \"lst\")\n\n\n\nthe conversion was successful and we now see the land surface temperature in Celsius, reaching dramatic values. We are set now to request a data cube from the CDSE with Sentinel-3 Data for the same region in time and space.\n\nWe once more use the stac-cdse store and set its credentials (consider the readme for information on how to obtain them).\n\ncdse_credentials = dict(\n    key=<your_key>,\n    secret=<your_secret>\n)\n\n\n\nThis time, we use the store with the identifier “stac-cdse-ardc” to ask for analysis-ready data cubes.\n\nstac_cdse_ardc_store = new_data_store(\"stac-cdse-ardc\", **cdse_credentials)\n\n\n\nNow that we have the store, we see what data it provides.\n\nstac_cdse_ardc_store.list_data_ids()\n\n\n\nAs it has Sentinel-3 Synergy Data. In addition to the region and the time range, we also specify which bands we want to open (given by asset_names), the coordinate reference system and the spatial resolution (both the same as the LST dataset). To get a fit with the LST data, we ask for the exact minimum and maximum spatial values. As assets, we ask for reflectance bands originating from the SLSTR instrument.\n\nmin_lon = min(converted_lst_ds.lon.values)\nmin_lat = min(converted_lst_ds.lat.values)\nmax_lon = max(converted_lst_ds.lon.values)\nmax_lat = max(converted_lst_ds.lat.values)\n\n\n\nsyn_ds = stac_cdse_ardc_store.open_data(\n    data_id=\"sentinel-3-syn-2-syn-ntc\",\n    bbox=[min_lon, min_lat, max_lon, max_lat],\n    time_range=southern_australia_event_time_range,\n    spatial_res=0.01,\n    crs=\"EPSG:4326\",\n    asset_names=[\"syn_S1N_reflectance\", \"syn_S2N_reflectance\"],\n)\nsyn_ds\n\n\n\nWhen the product is ready, we may continue with deriving an index from the reflectance bands.\n\nda = (syn_ds.SDR_S1N - syn_ds.SDR_S2N) / (syn_ds.SDR_S1N + syn_ds.SDR_S2N)\nsyn_ds = syn_ds.assign(vi=da)\nsyn_ds\n\n\n\nUltimately, we can plot the data the same way we plotted the LST data.\n\nplot_six_images(syn_ds, \"vi\")\n\n","type":"content","url":"/external-notebooks/esa-cci/ect-demonstrators/demonstrators/cdse/matching-lst-data-with-a-sentinel-3-cube","position":1},{"hierarchy":{"lvl1":"CDSE Demonstrators"},"type":"lvl1","url":"/external-notebooks/esa-cci/ect-demonstrators/demonstrators/cdse/readme","position":0},{"hierarchy":{"lvl1":"CDSE Demonstrators"},"content":"In this folder, we present how the \n\nESA CCI Toolbox\ncan be used together with Data from the Copernicus Dataspace Ecosystem (CDSE).\nThe CDSE provides Data and Services from the Copernicus Sentinel Missions.\nAs such, it suggests itself to show how to match it against ECV Data that\nhas been derived from Sentinel missions.\nWe also show how operations from the ESA CCI Toolbox or the underlying Python\ntechnology stack may be applied to improve the display or combination of data.\n\nThe demonstrator consists of two Jupyter Notebooks:\nIn the first one, we show how to match a FIRE dataset against single Sentinel-2 Observations,\nin the other one, we match LST Data with a Data Cube created from Sentinel-3 SLSTR Data.\nIn both cases, the CDSE Data is accessed using xcube data stores which are included in the\nxcube-stac plugin.\nTo obtain this - and other - required Python packages, please consider the environments.yml file\nwhich comes with the demonstrator and which may assist you with building a dedicated\nenvironment.\nIf you already have an environment, make sure you have the following components installed:\n\nesa-climate-toolbox\n\nxcube-stac\n\njupyterlab\n\nlibgdal-netcdf\n\nAlso, while access to the CDSE is free, you still need to register.\nIn particular, to use the stores, you will need to provide a set of credentials.\nTo do see, please go to \n\nhttps://​documentation​.dataspace​.copernicus​.eu​/APIs​/S3​.html​#registration\n\nWhen you have the credentials, you are set to execute the demonstrators.","type":"content","url":"/external-notebooks/esa-cci/ect-demonstrators/demonstrators/cdse/readme","position":1},{"hierarchy":{"lvl1":"Visualising Land Cover Class Change with Altair"},"type":"lvl1","url":"/external-notebooks/esa-cci/ect-demonstrators/demonstrators/d3-js/d3-js-lc-classes","position":0},{"hierarchy":{"lvl1":"Visualising Land Cover Class Change with Altair"},"content":"This notebook is one of three notebooks that serve to show how the ESA CCI Toolbox can be used with D3.js. Here, we will show how the classes of a Land Cover Dataset can be summed up and their change over time can be visualised in a D3.js chart accessed over the Altair library. Several preprocessing steps are applied, partly using operations from the CCI Toolbox.\n\nFor this notebook, we use the altair library. Install it into your Python environment if you haven’t already done so:\n\n# ! mamba install --yes altair\n\n\n\nNow we can make all the necessary imports.\n\nimport altair as alt\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom esa_climate_toolbox.core import get_op\nfrom esa_climate_toolbox.core import list_ecv_datasets\nfrom esa_climate_toolbox.core import open_data\n\n\n\nWe start by listing all the Land Cover Datasets. For more information on Land Cover Data, see here: \n\nhttps://​climate​.esa​.int​/en​/projects​/land​-cover/.\n\nlist_ecv_datasets(\"LC\")\n\n\n\nOf these, we open the one provided from the zarr store, using the toolbox’ open_data function. Note this method returns a tuple, consisting of the data and its identifier. We are only interested in the data here.\n\nlc_ds, _ = open_data(\"ESACCI-LC-L4-LCCS-Map-300m-P1Y-1992-2015-v2.0.7b.zarr\", data_store_id=\"esa-cci-zarr\")\nlc_ds\n\n\n\nThis dataset shows global land cover classes for the years from 1992 to 2015. The spatial is very large. We therefore want to focus on a smaller region, covering the Amazon rain forest. For subsetting, we use the toolbox’ subset_spatial operation. The region is passed in as a string in the form of “min_lon, min_lat, max_lon, max_lat”.\n\nsubset_spatial = get_op(\"subset_spatial\")\nlc_ds_sub = subset_spatial(lc_ds, region=\"-61, -5, -47, 2\")\nlc_ds_sub\n\n\n\nNow that we have made the dataset somewhat smaller, we may plot the data for the first and last timestep.\n\n%matplotlib inline\nlc_ds_sub.lccs_class.isel(time=0).plot(figsize=(10, 5))\n\n\n\nlc_ds_sub.lccs_class.isel(time=-1).plot(figsize=(10, 5))\n\n\n\nIt seems there are some differences. It might be good to look them up in more detail. If we look at the metadata of lccs_class, we see that flags and their meanings are defined. We have a look at the flag values first.\n\nlc_flag_values = np.array(lc_ds_sub.lccs_class.attrs.get(\"flag_values\"))\nlc_flag_values\n\n\n\nApparently, there was a mix-up with unsigned values. To account for this, we add the value 256 to all negative flag values in this list (not in the dataset itself, where the values are fine).\n\nlc_flag_values = np.where(lc_flag_values < 0, lc_flag_values + 256, lc_flag_values).astype(np.uint8)\nlc_flag_values\n\n\n\nThis is better. We now extract the flag meanings. As these are given in one big string, we split it whenver we encounter a space.\n\nlc_flag_meanings = lc_ds_sub.lccs_class.attrs.get(\"flag_meanings\")\nlc_flag_meanings = lc_flag_meanings.split(\" \")\nlc_flag_meanings\n\n\n\nNow we simply combine these to have pairs of flag values and flag meanings.\n\nlc_flags = [(lc_flag_values[i], lc_flag_meanings[i]) for i in range(len(lc_flag_values))]\nlc_flags\n\n\n\nAs the metadata is prepared, we can continue with preparing the LC dataset to be used with Altair. Now, as Altair expects pandas dataframes, we need to convert it. We could use the to_dataframe operation from the toolbox (which eventually is used in the other notebooks), but here we want to apply an additional step, where we count the number of occurences per class in each year. We therefore write a dedicated function. As this takes a little time to complete, we print out the time as means to measure the progress.\n\nlc_data = []\nfor i, t in enumerate(lc_ds_sub.time):\n    date = pd.Timestamp(t.values).strftime(\"%Y-%m-%d\")\n    print(date)\n    for lc_flag, lc_flag_name in lc_flags:\n        if lc_flag_name == \"no_data\":\n            continue\n        flag_count = (lc_ds_sub.isel(time=i)[\"lccs_class\"] == lc_flag).sum().values\n        lc_data.append({\n            \"date\": date,\n            \"lc_class\": lc_flag_name,\n            \"lc_count\": flag_count\n        })\ndf = pd.DataFrame(lc_data)\ndf\n\n\n\n\n\nWe end up with a dataframe that can be used with Altair. However, on closer inspection, there are a few classes that are not present in our subset. We remove these from the dataframe.\n\nfor lc_flag, lc_flag_name in lc_flags:\n    lc_count_sum = df[df[\"lc_class\"] == lc_flag_name].lc_count.sum()\n    if lc_count_sum == 0:\n        print(f\"Removing {lc_flag_name} from data frame\")\n        df = df[df[\"lc_class\"] != lc_flag_name]\ndf\n\n\n\n\n\nFinally, we can create a stacked chart from the Altair library. We assign the x-axis to display the date, the y-axis to show the occurences per class, and we assign a different color to each of the classes.\n\nalt.Chart(df).mark_area().encode(\n    x=\"date:T\",\n    y=alt.Y(\"lc_count:Q\"),\n    color=\"lc_class:N\"\n)\n\n\n\nA clear increase of cropland becomes visible, with a simultaneous decrease of natural herbaceous vegetation. Note that on the top right of the chart, you can open a menu that provides you with image export options, as well as an alternative insight into the data.","type":"content","url":"/external-notebooks/esa-cci/ect-demonstrators/demonstrators/d3-js/d3-js-lc-classes","position":1},{"hierarchy":{"lvl1":"Visualizing Cloud Fraction Uncertainties with Altair and Plotly"},"type":"lvl1","url":"/external-notebooks/esa-cci/ect-demonstrators/demonstrators/d3-js/d3-js-visualise-uncertainties","position":0},{"hierarchy":{"lvl1":"Visualizing Cloud Fraction Uncertainties with Altair and Plotly"},"content":"This notebook is one of three notebooks that serve to show how the ESA CCI Toolbox can be used with D3.js. Here, we will show how uncertainties included in a CCI Cloud Dataset can be visualised using charts from Altair and Plotly. Several preprocessing steps are applied, partly using operations from the CCI Toolbox.\n\nInstall altair and plotly it into your Python environment if you haven’t already done so:\n\n# ! mamba install --yes altair plotly\n\n\n\nAgain, we first take care of our necessary imports:\n\nimport altair as alt\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\n\nfrom esa_climate_toolbox.core import get_op\nfrom esa_climate_toolbox.core import list_ecv_datasets\nfrom esa_climate_toolbox.core import open_data\n\n\n\nFor this notebook, we want to look at a Cloud CCI dataset, so first we see what the toolbox has to offer here. For more information on Ozone Data, see here: \n\nhttps://​climate​.esa​.int​/en​/projects​/cloud/.\n\nlist_ecv_datasets(ecv=\"CLOUD\")\n\n\n\nThere is a zarr dataset covering data from 1982 to 2016, so we will continue with this. We open it using the toolbox’ open_data function.\n\ncloud_ds, _ = open_data(\"ESACCI-L3C_CLOUD-CLD_PRODUCTS-AVHRR_NOAA-1982-2016-fv3.0.zarr\", data_store_id=\"esa-cci-zarr\")\ncloud_ds\n\n\n\nWe see that it has a variable cfc which represents the cloud fraction. It comes also with several describin bands, such as bands describing the standard deviation and the uncertainty. This is perfect for our case.\nNow, we want to only look at the last 4 years, so we create a temporal subset using the subset_temporal_index operation.\n\nsubset_time_index = get_op(\"subset_temporal_index\")\ncloud_ds = subset_time_index(cloud_ds, 367, 415)\ncloud_ds\n\n\n\nIn the next step, we create a time series for a single point. We pick coordinate pointing at Hamburg, Germany.\n\ntpoint = get_op(\"tseries_point\")\ncloud_point_ts = tpoint(cloud_ds, point=\"10, 53.5\")\ncloud_point_ts\n\n\n\nWe receive a dataset that has only one value for lat and lon. This dataset can easily be converted into a dataframe, which will serve as input for the further visualisation. We use the to_dataframe function from the toolbox.\n\ndf_op = get_op(\"to_dataframe\")\ncloud_point_df = df_op(ds=cloud_point_ts)\ncloud_point_df\n\n\n\nThis method has automatically set the time as the dataframe index. This prevents time from being represented as a column, this making it unusable by Altair. As we don’t want this, we reset the index.\n\ncloud_point_df = cloud_point_df.reset_index().rename(columns={'index': 'timestamp'})\ncloud_point_df\n\n\n\nNow, the dataframe is ready to be used. We start with plotting a simple line using the Altair library. As we will use this later, we wrap it into a function. In the plot, the x-axis shows the time and the y-axis shows the cloud fraction.\n\ndef create_line(cloud_df: pd.DataFrame, color: str = \"blue\"):\n    return alt.Chart(cloud_df).mark_point(color=color).encode(\n        x='time:T',\n        y='cfc:Q'\n    )\n\n\n\nline = create_line(cloud_point_df)\nline\n\n\n\nNow, we can add the uncertainty values, which are encoded in the cfc_unc variable. We display these as a semi-transparent area, where the uncertainty values define top and bottom values around the cfc value. As the uncertainty is given in percent, we also need to divide it by 100.\n\ndef create_band(cloud_df: pd.DataFrame, color: str = \"blue\"):\n    return alt.Chart(cloud_df).mark_area(color=color, opacity=0.3).encode(\n        x='time:T',\n        y='y1:Q',\n        y2='y2:Q'\n    ).transform_calculate(\n        y1=\"datum.cfc - (datum.cfc_unc / 100 / 2)\",\n        y2=\"datum.cfc + (datum.cfc_unc / 100 / 2)\"\n    )\n\n\n\nband = create_band(cloud_point_df)\n\nline + band\n\n\n\nAlternatively, we can create bars to display variation. Here, we use the standard deviation band instead.\n\ndef create_bars(cloud_df: pd.DataFrame):\n    return alt.Chart(cloud_df).mark_rule().encode(\n        x='time:T',\n        y='cfc:Q',\n        y2=alt.Y('y_uncertainty:Q', scale=alt.Scale(zero=False), title=\"Uncertainty\")\n    ).transform_calculate(\n        y2=\"datum.cfc + (datum.cfc_std / 2)\",\n        y1=\"datum.cfc - (datum.cfc_std / 2)\"\n    ).encode(\n        y='y1:Q',\n        y2='y2:Q'\n    )\n\n\n\nbars = create_bars(cloud_point_df)\n\nline + bars\n\n\n\nWe may create such plots for another location, this time one in the Sahara desert ...\n\ncloud_point_2_ts = tpoint(cloud_ds, point=\"20, 25\")\ncloud_point_2_df = df_op(ds=cloud_point_2_ts)\ncloud_point_2_df = cloud_point_2_df.reset_index().rename(columns={'index': 'timestamp'})\nline_2 = create_line(cloud_point_2_df, color=\"red\")\nband_2 = create_band(cloud_point_2_df, color=\"red\")\n\nline_2 + band_2\n\n\n\n... or one in the Amazon rain forest.\n\ncloud_point_3_ts = tpoint(cloud_ds, point=\"-60, -10\")\ncloud_point_3_df = df_op(ds=cloud_point_3_ts)\ncloud_point_3_df = cloud_point_3_df.reset_index().rename(columns={'index': 'timestamp'})\nline_3 = create_line(cloud_point_3_df, color=\"green\")\nband_3 = create_band(cloud_point_3_df, color=\"green\")\n\nline_3 + band_3\n\n\n\nUltimately, we can display all plots together.\n\nline + band + line_2 + band_2 + line_3 + band_3\n\n\n\nNow, this works nicely for this dataset as it already has information on uncertainties and standard deviations. However, many CCI datasets don’t. In case you still want to get a number on this, you can create another time series, where you don’t get the value for a certain point, but the mean for a region, including the standard deviation. You may select suffixes for the means and standard deviations. In the case of the cloud datasets, this is necessary in order to avoid confusing the operation with the preexisting bands.\n\ntmean = get_op(\"tseries_mean\")\n\n\n\ncloud_mean_ts = tmean(cloud_ds, std_suffix=\"_stddev\")\ncloud_mean_ts\n\n\n\n\n\nNow, we consider a different case. Previously, we had a look at the uncertainty distribution over time for a single point. Here, we want to got the opposite approach, where we show the spatial distribution of uncertainty for one point in time. We do this using the plotly library.\n\nFirst, we select a time step. For this, we use the subset_temporal_index operation from the toolbox to use the latest timestep from the original cloud dataset.\n\nsubset_time_index = get_op(\"subset_temporal_index\")\nsub_cloud_ds = subset_time_index(cloud_ds, 47, 48)\nsub_cloud_ds\n\n\n\nWe receive a dataset with a single time step. As now we can neglect the time dimension, we do so by removing it.\n\nsub_cloud_ds = sub_cloud_ds.squeeze()\nsub_cloud_ds\n\n\n\nNext we convert the dataset to a dataframe.\n\ncloud_df = df_op(sub_cloud_ds)\ncloud_df\n\n\n\nThe resulting dataframe has a multi-index consisting of combinations of lat and lon. This is not a good input for us, as we want a table like view on the data. We therefore unstack the dataframe, effectively creating matrices of latitude and longitude values per data variable.\n\ncloud_df = cloud_df.unstack(level=\"lon\")\ncloud_df\n\n\n\nThis dataset is now fit to be visualised with plotly to create a 3D-surface plot. The x- and y-axes of this plot represent longitude and latitude. The z-axis shows the uncertainty, ahilst the actual cloud fraction is color-coded:\n\nfig = go.Figure(\n    data=[go.Surface(\n        x=cloud_ds.lon.values,\n        y=cloud_ds.lat.values,\n        z=cloud_df[[\"cfc_unc\"]].values,\n        surfacecolor=cloud_df[[\"cfc\"]].values,\n        colorbar=dict(title=\"Cloud Fraction\"),\n         hovertemplate=\n            \"Longitude: %{x}<br>\" +\n            \"Latitude: %{y}<br>\" +\n            \"Uncertainty in %: %{z}<extra></extra>\"  # Hoverinfo anpassen        \n    )]\n)\n\nfig.update_layout(title=dict(text='Visualizing Cloud Uncertainties'), autosize=False,\n                  width=500, height=500,\n                  margin=dict(l=65, r=50, b=65, t=90))\n\nfig.update_layout(\n    scene=dict(\n        xaxis_title=\"Longitude\",\n        yaxis_title=\"Latitude\",\n        zaxis_title=\"Uncertainty in %\"\n    )\n)\n\nfig.show()\n\n\n\n","type":"content","url":"/external-notebooks/esa-cci/ect-demonstrators/demonstrators/d3-js/d3-js-visualise-uncertainties","position":1},{"hierarchy":{"lvl1":"Visualizing 4-dimensional Data with Altair and Plotly"},"type":"lvl1","url":"/external-notebooks/esa-cci/ect-demonstrators/demonstrators/d3-js/d3-js-visualize-4d-data","position":0},{"hierarchy":{"lvl1":"Visualizing 4-dimensional Data with Altair and Plotly"},"content":"This notebook is one of three notebooks that serve to show how the ESA CCI Toolbox can be used with D3.js. Here, we will show how CCI Ozone Data, which has a fourth dimension beyond time. latitude, and longitude, can be visualised using charts from Altair and Plotly. Several preprocessing steps are applied, partly using operations from the CCI Toolbox.\n\nInstall altair and plotly it into your Python environment if you haven’t already done so:\n\n# ! mamba install --yes altair plotly\n\n\n\nimport altair as alt\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport math\nimport numpy as np\nimport pandas as pd\n\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfrom esa_climate_toolbox.core import get_op\nfrom esa_climate_toolbox.core import list_ecv_datasets\nfrom esa_climate_toolbox.core import open_data\n\n\n\nOnce more, we start with listing the datasets that are potentially useful for our purpose. For more information on Ozone Data, visit \n\nhttps://​climate​.esa​.int​/en​/projects​/ozone/.\n\nlist_ecv_datasets(ecv=\"OZONE\")\n\n\n\nHere, no zarr or kerchunk datasets are available, so we use data from the odp store. We pick the multi-sensor multi-platform dataset in a monthly resolution. We delimit the number of opened variables, so we only pick the weighted ozone average, and choose a temporal subset.\n\nozone_ds, _ = open_data(\n    \"esacci.OZONE.mon.L3.NP.multi-sensor.multi-platform.MERGED.fv0002.r1\", \n    var_names=\"O3_du\",\n    data_store_id=\"esa-cci\"\n)\nozone_ds\n\n\n\nThe variable O3_du has a fourth dimension, layers. The dataset still has a dimension air_pressure that is not needed anymore, as no variable makes use of it, so we drop it.\n\nozone_ds = ozone_ds.drop_dims(\"air_pressure\")\nozone_ds\n\n\n\nsubset_time_index = get_op(\"subset_temporal_index\")\nozone_sub_ds = subset_time_index(ozone_ds, 120, 144)\nozone_sub_ds\n\n\n\nWe create a time series for a point, given through a string encoded as “longitude, latitude”.\n\ntpoint = get_op(\"tseries_point\")\nozone_point_ts = tpoint(ozone_sub_ds, point=\"10, 53.5\")\nozone_point_ts\n\n\n\nWe now convert this to a dataframe that may be read from the visualisation libraries.\n\ndf_op  = get_op(\"to_dataframe\")\nozone_point_ts_df = df_op(ozone_point_ts)\nozone_point_ts_df\n\n\n\nThe resulting dataframe has a multi-index that is inconvenient to use, so we resolve it to be able to address the columns stating the index.\n\nozone_point_ts_df = ozone_point_ts_df.reset_index().rename(columns={'index': 'timestamp'})\nozone_point_ts_df\n\n\n\nWe now create a chart from this dataframe, where each layer dimension receives a different color, allowing to visualize them in the same chart.\n\nalt.Chart(ozone_point_ts_df).mark_line().encode(\n    x='time:T',\n    y='O3_du:Q',\n    color='layers:N'\n)\n\n\n\nLike in the notebook on visualizing uncertainties, we are not only interested in showing the layer dimension of a time-series without spatial extent, but also of a grid for a single timestep. For this, we select the last time step ot the ozone dataset. For this, we again use plotly. Whilst we created a single plot in the previous notebook, we will now go on to create one surface area per layer dimension.\n\nsubset_time_index = get_op(\"subset_temporal_index\")\nozone_sub_ds = subset_time_index(ozone_ds, 143, 144)\nozone_sub_ds\n\n\n\nWe want to remove the time dimension from the dataset so we have a leaner data array to work with.\n\nozone_sub_ds = ozone_sub_ds.squeeze()\nozone_sub_ds\n\n\n\nGood. Now this is done, we can prepare the plots. There are sixteen layers, so we set up a structure of 8 rows and 2 columns.\n\nfig = make_subplots(\n    rows=8,     \n    cols=2,\n    subplot_titles=(\n        \"Layer 1\", \"Layer 2\", \"Layer 3\", \"Layer 4\",\n        \"Layer 5\", \"Layer 6\", \"Layer 7\", \"Layer 8\",\n        \"Layer 9\", \"Layer 10\", \"Layer 11\", \"Layer 12\",\n        \"Layer 13\", \"Layer 14\", \"Layer 15\", \"Layer 16\"\n    ),\n    specs=[[{'type': 'surface'}, {'type': 'surface'}],\n           [{'type': 'surface'}, {'type': 'surface'}],\n           [{'type': 'surface'}, {'type': 'surface'}],\n           [{'type': 'surface'}, {'type': 'surface'}],\n           [{'type': 'surface'}, {'type': 'surface'}],\n           [{'type': 'surface'}, {'type': 'surface'}],\n           [{'type': 'surface'}, {'type': 'surface'}],\n           [{'type': 'surface'}, {'type': 'surface'}]\n          ]    \n)\n\n\n\nWe can now create the surfaces. In each surface, latitude and longitude will stand for the x and y dimensions. The z-values will show the ozone average for the layer in question.\n\ny = ozone_sub_ds.lat.values\nx = ozone_sub_ds.lon.values\n\nfor i in range(len(ozone_sub_ds.layers)):\n    r = math.floor(i / 2) + 1    \n    c = math.floor(i % 2) + 1\n    print(f\"Creating plot for layer at {r}, {c}\")\n    z = ozone_sub_ds.O3_du.isel(layers=i).values\n    layer_number = ozone_sub_ds.layers.isel(layers=i).values\n    fig.add_trace(\n        go.Surface(x=x, y=y, z=z, showscale=False, name=f\"Layer {layer_number}\"),\n        row=r, \n        col=c,        \n    )\nfig.update_layout(\n    title_text='Different layers of mole content of ozone in the different atmosphere layers',\n    height=1600,\n    width=1200,\n    showlegend=True\n)\nfig.update_layout(margin=dict(t=100))\nfig.show()\n\n\n\n\n\n","type":"content","url":"/external-notebooks/esa-cci/ect-demonstrators/demonstrators/d3-js/d3-js-visualize-4d-data","position":1},{"hierarchy":{"lvl1":"D3.js Demonstrators"},"type":"lvl1","url":"/external-notebooks/esa-cci/ect-demonstrators/demonstrators/d3-js/readme","position":0},{"hierarchy":{"lvl1":"D3.js Demonstrators"},"content":"In this folder, we present how the \n\nESA CCI Toolbox\ncan be used together with D3.js.\nD3.js, or D3, is short for Data Driven Documents.\nIt is a open-source JavaScript library for visualizing data.\nIt was originally created in 2011 and has since been found ample use,\nand it has been extended by many contributors.\nD3.js is a low-level library.\nIt is extremely flexible, allowing users to define exactly the kind of visualisation\nthey have in mind.\nWhilst this approach makes sense for organizations that receive very large audiences\nor that have a large team of editors behind it, in most cases it makes sense to use\none of the many visualisation libraries or tools that have been built on top of D3.js.\nSuch libraries have been designed to provide a fast and user-friendly means to create\ncustomizable and re-usable designs.\nFor the purpose of this demonstrator, we found it makes sense to use these libraries,\nas they show the easiest way to use D3.js with CCI Data accessed through and processed\nby the CCI Toolbox, thereby allowing and encouraging users to create their own\nvisualisations - due to the flexibility of D3.js, these may be of arbitrary complexity.","type":"content","url":"/external-notebooks/esa-cci/ect-demonstrators/demonstrators/d3-js/readme","position":1},{"hierarchy":{"lvl1":"D3.js Demonstrators","lvl2":"Vega/Altair"},"type":"lvl2","url":"/external-notebooks/esa-cci/ect-demonstrators/demonstrators/d3-js/readme#vega-altair","position":2},{"hierarchy":{"lvl1":"D3.js Demonstrators","lvl2":"Vega/Altair"},"content":"We will use two different libraries built on D3.js.\nThe first one is \n\nAltair.\nAltair is a declarative visualisation library which provides a Python API for\n\n\nVega-Lite, a high-level grammar of interactive\ngraphics. Vega-Lite in turn is based on \n\nVega, which\nis a language for visualisation designs that\n\n\nheavily uses D3.js.\nVega and Vega-Lite are used by the\n\n\nxcube-viewer, so for more examples on the\nvisualisations that can be achieved with D3.js, we’d like to refer you to the\nsections of the ESA CCI Toolbox Documentation that handle the usage of the xcube viewer\nwith the Toolbox:\n\nhttps://​esa​-climate​-toolbox​.readthedocs​.io​/en​/latest​/notebooks​/Advanced​_Functionality​/1​-ECT​_Using​_xcube​_viewer​.html\n\nhttps://​esa​-climate​-toolbox​.readthedocs​.io​/en​/latest​/viewer​.html","type":"content","url":"/external-notebooks/esa-cci/ect-demonstrators/demonstrators/d3-js/readme#vega-altair","position":3},{"hierarchy":{"lvl1":"D3.js Demonstrators","lvl2":"Plotly"},"type":"lvl2","url":"/external-notebooks/esa-cci/ect-demonstrators/demonstrators/d3-js/readme#plotly","position":4},{"hierarchy":{"lvl1":"D3.js Demonstrators","lvl2":"Plotly"},"content":"The other library introduced in this demonstrator is\n\n\nplotly.\nPlotly is the Python version of the high-level charting library plotly.js, which bases\ndirectly on D3.js.\nPlotly offers a range of charts and visualisation types that may be used in Jupyter\nNotebooks or HTML pages.","type":"content","url":"/external-notebooks/esa-cci/ect-demonstrators/demonstrators/d3-js/readme#plotly","position":5},{"hierarchy":{"lvl1":"D3.js Demonstrators","lvl2":"Notebooks"},"type":"lvl2","url":"/external-notebooks/esa-cci/ect-demonstrators/demonstrators/d3-js/readme#notebooks","position":6},{"hierarchy":{"lvl1":"D3.js Demonstrators","lvl2":"Notebooks"},"content":"Using these libraries, we created three notebooks.\nIn the first one, we show an advanced example on the change of land cover class\nmembership. The second notebook portrays how uncertainties may be visualised, based on\ncloud data, whilst the last one handles the issue of multi-dimensional data, exemplified\nby ozone data with a profile dimension.","type":"content","url":"/external-notebooks/esa-cci/ect-demonstrators/demonstrators/d3-js/readme#notebooks","position":7}]}